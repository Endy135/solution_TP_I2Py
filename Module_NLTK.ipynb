{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: cyan; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<br/><br/>\n",
    "<h3 style=\"font-size: 35px; color: white; align: center;\"><center>\n",
    "<img src=\"https://th.bing.com/th/id/OIP._sf83AYB5O_8l6pVD3EQkgAAAA?w=140&h=150&c=7&r=0&o=5&dpr=1.25&pid=1.7\n",
    "\" width=\"150px\" /><br/><br/>\n",
    "        <h1 style=\"color:pink;\">LE MODULE NLTK</h1>\n",
    "    <h5 style=\"color:black;\">Présentée par DIANE NOUNGANG</h5>\n",
    " </center></h3>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>I-INTRODUCTION</h1></center>\n",
    "\n",
    "   <h3>Q'est ce que le Natural LAnguage Processing(NLP)</h3>\n",
    "  \n",
    "  La méthode de communication à l'aide de laquelle les humains peuvent parler, lire et écrire,\n",
    "est la langue.En d'autres termes, nous, les humains, pouvons penser, faire des plans, prendre des décisions dans notre langage naturel. Ici la grande question est: à l'ère de l'intelligence artificielle, machine\n",
    "apprentissage et apprentissage en profondeur, les humains peuvent-ils communiquer en langage naturel avec les \n",
    "ordinateurs/machines? Développer des applications NLP est un énorme défi pour nous car\n",
    "les ordinateurs ont besoin de données structurées, mais d'un autre côté, la parole humaine n'est pas structurée\n",
    "et souvent de nature ambiguë.voici quelques listes des outils NLP:\n",
    " ~Natural Language Tool Kit (NLTK)\n",
    " ~Mallet\n",
    " ~GATE\n",
    " ~Open NLP\n",
    " ~UIMA\n",
    " ~Genism\n",
    "\n",
    "   <h3>Natural Language Tool Kit (NLTK)</h3>\n",
    "  \n",
    "  Parmi les outils NLP mentionnés ci-dessus, NLTK obtient des scores très élevés en ce qui concerne la facilité\n",
    "d'utilisation et explication du concept. La courbe d'apprentissage de Python est très rapide et NLTK\n",
    "est écrit en Python donc NLTK a aussi un très bon kit d'apprentissage. NLTK a incorporé la plupart des tâches comme la tokenisation(tokenization), la radicalisation(stemming), la lemmatisation(Lemmatization), la ponctuation(punctuation), le caractère Comptez(caracter Count) et comptez les mots(Word Count). Il est très élégant et facile à travailler.\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>II-Installation du module NLTK</h1></center>\n",
    "  <h3>Sous Windows</h3>\n",
    "  \n",
    "      Tout d'abord, ouvrez l'invite de commande Windows et accédez à l'emplacement du pip dossier.\n",
    "      Ensuite, entrez la commande suivante pour installer l'invite NLTK:\n",
    "  \n",
    "      pip3 install nltk\n",
    " \n",
    " <h3>Sous Mac/Linux</h3>\n",
    " \n",
    "     sudo pip install -U nltk\n",
    "     Si vous navez pas le pip,faites comme suit:\n",
    "     1) sudo apt update\n",
    "     2) sudo apt install python3-pip\n",
    " \n",
    " <h3>Téléchargement de l'ensemble de données et des packages de NLTK</h3>\n",
    " \n",
    "     1) ouvrir un terminal\n",
    "     2) import nltk\n",
    "     3) nltk.download()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> III— Tokenisation de text</h1></center>\n",
    " <h3>Qu'est-ce que la tokenisation ?</h3>\n",
    "     il peut être défini comme le processus consistant à diviser un morceau de texte en parties plus petites, telles que phrases et mots. Ces parties plus petites sont appelées jetons. Par exemple, un mot est un jeton dans une phrase, et une phrase est un jeton dans un paragraphe.\n",
    "     Comme nous savons que le NLP est utilisé pour créer des applications telles que l'analyse des sentiments, les systèmes d'assurance qualité,traduction linguistique, chatbots intelligents, systèmes vocaux, etc., donc, pour les construire,il devient vital de comprendre le modèle dans le texte. Les jetons, mentionnés ci-dessus, sont très utile pour trouver et comprendre ces modèles. On peut considérer la tokenisation comme l'étape de base pour d'autres recettes telles que le stemming et la lemmatisation.\n",
    "     <h5>nltk.tokenize</h5>est le package fourni par le module NLTK pour réaliser le processus de\n",
    "tokenisation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Tokeniser des phrases en mots</h5>\n",
    "On va pour cela utiliser le module * word_tokenize * \n",
    "<h3>Exemple</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/noungang/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-26b7a65010dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DianaN.com pour ceux qui veulent bien travailler durement.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#on aura comme sortie ['DianaN.com','pour','ceux','qui','veulent','bien','travailler','durement','.']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/noungang/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('DianaN.com pour ceux qui veulent bien travailler durement.')\n",
    "\n",
    "#on aura comme sortie ['DianaN.com','pour','ceux','qui','veulent','bien','travailler','durement','.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>La classe TreebankWordTokenizer</h5>\n",
    "Le module *~ word_tokenize ~*, utilisé ci-dessus est essentiellement une fonction wrapper qui appelle   *~tokenize()~* fonctionner comme une instance de la classe TreebankWordTokenizer.Il donnera la même boîte à outils de traitement du langage naturel sortie que nous obtenons en utilisant le module word_tokenize() pour diviser les phrases en mot.\n",
    "Voyons le même exemple implémenté ci-dessus :\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "Tokenizer_wrd = TreebankWordTokenizer()\n",
    "Tokenizer_wrd.tokenize('DianaN.com pour ceux qui veulent bien travailler durement.')\n",
    "\n",
    "#on aura toujours ['DianaN.com','pour','ceux','qui','veulent','bien','travailler','durement','.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convention la plus importante d'un tokenizer est de séparer les contractions. Par exemple, si\n",
    "nous utilisons le module word_tokenize() à cette fin, il donnera la sortie comme suit :\n",
    "<h3>Exemple:</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('won’t')\n",
    "\n",
    "##sortie['wo', \"n't\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Tokénisation de phrases à l'aide d'expressions régulières<h5>\n",
    "    <h3>Exemple:</h3>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"il y a n'est pas un mot.\")\n",
    "\n",
    "#sortie['il','y','a',\"n'est\",'pas','un','mot','.']\n",
    "#Dans cette exemple, nous utiliserons une expression régulière pour marquer les espaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>IV Recherche de mots dans Wordnet Boîte à outils de traitement du langage naturel</h1></center>\n",
    "<h5>Qu'est ce que le Wornet</h5>\n",
    "    Wordnet est une grande base de données lexicale de l'anglais, qui a été créée par Princeton. C'est une partie\n",
    "du corpus NLTK. Les noms, verbes, adjectifs et adverbes sont tous regroupés en ensemble de synsets, c'est-à-dire des synonymes cognitifs. Ici, chaque ensemble de synsets exprime une signification distincte.Voici quelques cas d'utilisation de Wordnet :\n",
    "    \n",
    "    -Il peut être utilisé pour rechercher la définition d'un mot\n",
    "    -On peut trouver des synonymes et des antonymes d'un mot\n",
    "    -Les relations et les similitudes de mots peuvent être explorées à l'aide de Wordnet\n",
    "    -Désambiguïsation du sens des mots pour les mots ayant plusieurs utilisations et définitions\n",
    "   <h5>comment importer le wordnet</h5>\n",
    "   Il peut etre importer comme suit:\n",
    "   from nltk.corpus import wordnet\n",
    "   \n",
    "   Pour une commande plus compacte, utilisez ce qui suit :\n",
    "   from nltk.corpus import wordnet as wn\n",
    "   \n",
    "   <h3>Exemple1:<h3>\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "syn = wn.synsets('dog')[0]\n",
    "syn.name()#utilisation de la methode name()\n",
    "\n",
    "#sortie: 'dog.n.01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Ensuite, nous utilisons la méthode *definition()* qui nous donnera la définition du mot :</h5>\n",
    "  <h3>Exemple2:<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "syn = wn.synsets('dog')[0]\n",
    "syn.definition()\n",
    "\n",
    "#sortie 'un membre du genre Canis (probablement descendant du loup commun) qui a été domestiqué par l'homme depuis la préhistoire ; se produit dans de nombreuses races'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmes dans Wordnet</h3>\n",
    "    En linguistique, la forme canonique ou la forme morphologique d'un mot s'appelle un lemme. À trouver un synonyme ainsi qu'un antonyme d'un mot, nous pouvons également rechercher des lemmes dans WordNet. Laisser nous voyons comment.\n",
    "    <h6>Trouver des synonymes</h6>\n",
    "    En utilisant la méthode lemme(), nous pouvons trouver le nombre de synonymes d'un Synset. Laissez-nous appliquer cette méthode sur le synset 'dog':\n",
    "    <h3>Example3</h3>\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "syn = wn.synsets('dog')[0]\n",
    "lemmas = syn.lemmas()\n",
    "len(lemmas)       #sortie 3\n",
    "lemmas[0].name()  #sorie 'dog'\n",
    "lemmas[1].name()  #sortie 'domestic_dog'\n",
    "lemmas[2].name()  #'Canis_familiaris'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>V-Stemming & Lemmatisation</h1></center>\n",
    "    <h3>Le Stemming,qu'est ce que?</h3>\n",
    "    Le stemming est une technique utilisée pour extraire la forme de base des mots en supprimant les affixes d'eux. C'est comme couper les branches d'un arbre jusqu'à ses tiges. Par exemple,la racine des mots manger, mange, mangé est manger.\n",
    "    Les moteurs de recherche utilisent le stemming pour indexer les mots. C'est pourquoi plutôt que de tout stocker\n",
    "formes d'un mot, un moteur de recherche ne peut stocker que les radicaux. De cette manière, le stemming réduit\n",
    "la taille de l'index et augmente la précision de la récupération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Methodes  de Stemming</h3>\n",
    "Dans NLTK, stemmerI, qui a la méthode stem(), avec les stemmers: PorterStemmer,LancasterStem,RegrexpStemmer,SnowballStemmer.\n",
    "<h5>la classe PorterStemmer<h5>\n",
    "    NLTK a la classe PorterStemmer à l'aide de laquelle nous pouvons facilement implémenter Porter Algorithmes de radicalisation pour le mot que nous voulons radicaliser. Cette classe connaît plusieurs mots réguliers\n",
    "formes et suffixes à l'aide desquels il peut transformer le mot d'entrée en une racine finale.\n",
    "La racine résultante est souvent un mot plus court ayant la même signification racine.\n",
    "    <h3>Exemple:</h3>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('writing')\n",
    "\n",
    "#sortie 'write'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>La classe SnowballStemmer</h5>\n",
    "    NLTK a la classe SnowballStemmer à l'aide de laquelle nous pouvons facilement implémenter Algorithmes Snowball Stemmer. Il prend en charge 15 langues autres que l'anglais. Afin d'utiliser ce classe steaming, nous devons créer une instance avec le nom de la langue que nous utilisons\n",
    "puis appelez la méthode stem(). Voyons un exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages #pour voir les langues qu'il prend en charge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, créez une instance de la classe SnowballStemmer avec le langage que vous souhaitez utiliser.\n",
    "Ici, nous créons le stemmer pour la langue *french*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "French_stemmer = SnowballStemmer(‘french’)\n",
    "French_stemmer.stem (‘Bonjoura’)\n",
    "\n",
    "#sortie 'Bonjour'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>-Lemmitisation</h1>\n",
    "<h3>Qu'est-ce que la lemmatisation </h3>\n",
    "La technique de lemmatisation est comme le stemming. La sortie que nous obtiendrons après lemmatisation est\n",
    "appelé «lemme», qui est un mot racine plutôt qu'une racine racine, la sortie de la racine. Après\n",
    "lemmatisation, nous obtiendrons un mot valide qui signifie la même chose.\n",
    "\n",
    "NLTK fournit la classe WordNetLemmatizer qui est une mince enveloppe autour du wordnet\n",
    "corpus. Cette classe utilise la fonction morphy() de la classe WordNet CorpusReader pour trouver\n",
    "un lemme. Comprenons-le avec un exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('books')\n",
    "#sortie 'book'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>toutes les methodes de loutil NLTK</h1>\n",
    "\ta\t\n",
    "- \tnltk.app\t\n",
    "\t    nltk.app.chartparser_app\t\n",
    "\t    nltk.app.chunkparser_app\t\n",
    "\t    nltk.app.collocations_app\t\n",
    "\t    nltk.app.concordance_app\t\n",
    "\t    nltk.app.nemo_app\t\n",
    "\t    nltk.app.rdparser_app\t\n",
    "\t    nltk.app.srparser_app\t\n",
    "\t    nltk.app.wordfreq_app\t\n",
    "\t    nltk.app.wordnet_app\t\n",
    "\t \t\n",
    "\tb\t\n",
    "\tnltk.book\t\n",
    "\t \t\n",
    "\tc\t\n",
    "- \tnltk.ccg\t\n",
    "\t    nltk.ccg.api\t\n",
    "\t    nltk.ccg.chart\t\n",
    "\t    nltk.ccg.combinator\t\n",
    "\t    nltk.ccg.lexicon\t\n",
    "\t    nltk.ccg.logic\t\n",
    "- \tnltk.chat\t\n",
    "\t    nltk.chat.eliza\t\n",
    "\t    nltk.chat.iesha\t\n",
    "\t    nltk.chat.rude\t\n",
    "\t    nltk.chat.suntsu\t\n",
    "\t    nltk.chat.util\t\n",
    "\t    nltk.chat.zen\t\n",
    "- \tnltk.chunk\t\n",
    "\t    nltk.chunk.api\t\n",
    "\t    nltk.chunk.named_entity\t\n",
    "\t    nltk.chunk.regexp\t\n",
    "\t    nltk.chunk.util\t\n",
    "- \tnltk.classify\t\n",
    "\t    nltk.classify.api\t\n",
    "\t    nltk.classify.decisiontree\t\n",
    "\t    nltk.classify.maxent\t\n",
    "\t    nltk.classify.megam\t\n",
    "\t    nltk.classify.naivebayes\t\n",
    "\t    nltk.classify.positivenaivebayes\t\n",
    "\t    nltk.classify.rte_classify\t\n",
    "\t    nltk.classify.scikitlearn\t\n",
    "\t    nltk.classify.senna\t\n",
    "\t    nltk.classify.svm\t\n",
    "\t    nltk.classify.tadm\t\n",
    "\t    nltk.classify.textcat\t\n",
    "\t    nltk.classify.util\t\n",
    "\t    nltk.classify.weka\t\n",
    "\tnltk.cli\t\n",
    "- \tnltk.cluster\t\n",
    "\t    nltk.cluster.api\t\n",
    "\t    nltk.cluster.em\t\n",
    "\t    nltk.cluster.gaac\t\n",
    "\t    nltk.cluster.kmeans\t\n",
    "\t    nltk.cluster.util\t\n",
    "\tnltk.collections\t\n",
    "\tnltk.collocations\t\n",
    "\tnltk.compat\t\n",
    "- \tnltk.corpus\t\n",
    "\t    nltk.corpus.europarl_raw\t\n",
    "\t    nltk.corpus.reader\t\n",
    "\t    nltk.corpus.reader.aligned\t\n",
    "\t    nltk.corpus.reader.api\t\n",
    "\t    nltk.corpus.reader.bnc\t\n",
    "\t    nltk.corpus.reader.bracket_parse\t\n",
    "\t    nltk.corpus.reader.categorized_sents\t\n",
    "\t    nltk.corpus.reader.chasen\t\n",
    "\t    nltk.corpus.reader.childes\t\n",
    "\t    nltk.corpus.reader.chunked\t\n",
    "\t    nltk.corpus.reader.cmudict\t\n",
    "\t    nltk.corpus.reader.comparative_sents\t\n",
    "\t    nltk.corpus.reader.conll\t\n",
    "\t    nltk.corpus.reader.crubadan\t\n",
    "\t    nltk.corpus.reader.dependency\t\n",
    "\t    nltk.corpus.reader.framenet\t\n",
    "\t    nltk.corpus.reader.ieer\t\n",
    "\t    nltk.corpus.reader.indian\t\n",
    "\t    nltk.corpus.reader.ipipan\t\n",
    "\t    nltk.corpus.reader.knbc\t\n",
    "\t    nltk.corpus.reader.lin\t\n",
    "\t    nltk.corpus.reader.mte\t\n",
    "\t    nltk.corpus.reader.nkjp\t\n",
    "\t    nltk.corpus.reader.nombank\t\n",
    "\t    nltk.corpus.reader.nps_chat\t\n",
    "\t    nltk.corpus.reader.opinion_lexicon\t\n",
    "\t    nltk.corpus.reader.panlex_lite\t\n",
    "\t    nltk.corpus.reader.panlex_swadesh\t\n",
    "\t    nltk.corpus.reader.pl196x\t\n",
    "\t    nltk.corpus.reader.plaintext\t\n",
    "\t    nltk.corpus.reader.ppattach\t\n",
    "\t    nltk.corpus.reader.propbank\t\n",
    "\t    nltk.corpus.reader.pros_cons\t\n",
    "\t    nltk.corpus.reader.reviews\t\n",
    "\t    nltk.corpus.reader.rte\t\n",
    "\t    nltk.corpus.reader.semcor\t\n",
    "\t    nltk.corpus.reader.senseval\t\n",
    "\t    nltk.corpus.reader.sentiwordnet\t\n",
    "\t    nltk.corpus.reader.sinica_treebank\t\n",
    "\t    nltk.corpus.reader.string_category\t\n",
    "\t    nltk.corpus.reader.switchboard\t\n",
    "\t    nltk.corpus.reader.tagged\t\n",
    "\t    nltk.corpus.reader.timit\t\n",
    "\t    nltk.corpus.reader.toolbox\t\n",
    "\t    nltk.corpus.reader.twitter\t\n",
    "\t    nltk.corpus.reader.udhr\t\n",
    "\t    nltk.corpus.reader.util\t\n",
    "\t    nltk.corpus.reader.verbnet\t\n",
    "\t    nltk.corpus.reader.wordlist\t\n",
    "\t    nltk.corpus.reader.wordnet\t\n",
    "\t    nltk.corpus.reader.xmldocs\t\n",
    "\t    nltk.corpus.reader.ycoe\t\n",
    "\t    nltk.corpus.util\t\n",
    "\t \t\n",
    "\td\t\n",
    "\tnltk.data\t\n",
    "\tnltk.decorators\t\n",
    "\tnltk.downloader\t\n",
    "- \tnltk.draw\t\n",
    "\t    nltk.draw.cfg\t\n",
    "\t    nltk.draw.dispersion\t\n",
    "\t    nltk.draw.table\t\n",
    "\t    nltk.draw.tree\t\n",
    "\t    nltk.draw.util\t\n",
    "\t \t\n",
    "\tf\t\n",
    "\tnltk.featstruct\t\n",
    "\t \t\n",
    "\tg\t\n",
    "\tnltk.grammar\t\n",
    "\t \t\n",
    "\th\t\n",
    "\tnltk.help\t\n",
    "\t \t\n",
    "\ti\t\n",
    "- \tnltk.inference\t\n",
    "\t    nltk.inference.api\t\n",
    "\t    nltk.inference.discourse\t\n",
    "\t    nltk.inference.mace\t\n",
    "\t    nltk.inference.nonmonotonic\t\n",
    "\t    nltk.inference.prover9\t\n",
    "\t    nltk.inference.resolution\t\n",
    "\t    nltk.inference.tableau\t\n",
    "\tnltk.internals\t\n",
    "\t \t\n",
    "\tj\t\n",
    "\tnltk.jsontags\t\n",
    "\t \t\n",
    "\tl\t\n",
    "\tnltk.lazyimport\t\n",
    "- \tnltk.lm\t\n",
    "\t    nltk.lm.api\t\n",
    "\t    nltk.lm.counter\t\n",
    "\t    nltk.lm.models\t\n",
    "\t    nltk.lm.preprocessing\t\n",
    "\t    nltk.lm.smoothing\t\n",
    "\t    nltk.lm.util\t\n",
    "\t    nltk.lm.vocabulary\t\n",
    "\t \t\n",
    "\tm\t\n",
    "- \tnltk.metrics\t\n",
    "\t    nltk.metrics.agreement\t\n",
    "\t    nltk.metrics.aline\t\n",
    "\t    nltk.metrics.association\t\n",
    "\t    nltk.metrics.confusionmatrix\t\n",
    "\t    nltk.metrics.distance\t\n",
    "\t    nltk.metrics.paice\t\n",
    "\t    nltk.metrics.scores\t\n",
    "\t    nltk.metrics.segmentation\t\n",
    "\t    nltk.metrics.spearman\t\n",
    "- \tnltk.misc\t\n",
    "\t    nltk.misc.babelfish\t\n",
    "\t    nltk.misc.chomsky\t\n",
    "\t    nltk.misc.minimalset\t\n",
    "\t    nltk.misc.sort\t\n",
    "\t    nltk.misc.wordfinder\t\n",
    "\t \t\n",
    "\tn\t\n",
    "\tnltk\t\n",
    "\t \t\n",
    "\tp\t\n",
    "- \tnltk.parse\t\n",
    "\t    nltk.parse.api\t\n",
    "\t    nltk.parse.bllip\t\n",
    "\t    nltk.parse.chart\t\n",
    "\t    nltk.parse.corenlp\t\n",
    "\t    nltk.parse.dependencygraph\t\n",
    "\t    nltk.parse.earleychart\t\n",
    "\t    nltk.parse.evaluate\t\n",
    "\t    nltk.parse.featurechart\t\n",
    "\t    nltk.parse.generate\t\n",
    "\t    nltk.parse.malt\t\n",
    "\t    nltk.parse.nonprojectivedependencyparser\t\n",
    "\t    nltk.parse.pchart\t\n",
    "\t    nltk.parse.projectivedependencyparser\t\n",
    "\t    nltk.parse.recursivedescent\t\n",
    "\t    nltk.parse.shiftreduce\t\n",
    "\t    nltk.parse.stanford\t\n",
    "\t    nltk.parse.transitionparser\t\n",
    "\t    nltk.parse.util\t\n",
    "\t    nltk.parse.viterbi\t\n",
    "\tnltk.probability\t\n",
    "\t \t\n",
    "\ts\t\n",
    "- \tnltk.sem\t\n",
    "\t    nltk.sem.boxer\t\n",
    "\t    nltk.sem.chat80\t\n",
    "\t    nltk.sem.cooper_storage\t\n",
    "\t    nltk.sem.drt\t\n",
    "\t    nltk.sem.drt_glue_demo\t\n",
    "\t    nltk.sem.evaluate\t\n",
    "\t    nltk.sem.glue\t\n",
    "\t    nltk.sem.hole\t\n",
    "\t    nltk.sem.lfg\t\n",
    "\t    nltk.sem.linearlogic\t\n",
    "\t    nltk.sem.logic\t\n",
    "\t    nltk.sem.relextract\t\n",
    "\t    nltk.sem.skolemize\t\n",
    "\t    nltk.sem.util\t\n",
    "- \tnltk.sentiment\t\n",
    "\t    nltk.sentiment.sentiment_analyzer\t\n",
    "\t    nltk.sentiment.util\t\n",
    "\t    nltk.sentiment.vader\t\n",
    "- \tnltk.stem\t\n",
    "\t    nltk.stem.api\t\n",
    "\t    nltk.stem.arlstem\t\n",
    "\t    nltk.stem.arlstem2\t\n",
    "\t    nltk.stem.cistem\t\n",
    "\t    nltk.stem.isri\t\n",
    "\t    nltk.stem.lancaster\t\n",
    "\t    nltk.stem.porter\t\n",
    "\t    nltk.stem.regexp\t\n",
    "\t    nltk.stem.rslp\t\n",
    "\t    nltk.stem.snowball\t\n",
    "\t    nltk.stem.util\t\n",
    "\t    nltk.stem.wordnet\t\n",
    "\t \t\n",
    "\tt\t\n",
    "- \tnltk.tag\t\n",
    "\t    nltk.tag.api\t\n",
    "\t    nltk.tag.brill\t\n",
    "\t    nltk.tag.brill_trainer\t\n",
    "\t    nltk.tag.crf\t\n",
    "\t    nltk.tag.hmm\t\n",
    "\t    nltk.tag.hunpos\t\n",
    "\t    nltk.tag.mapping\t\n",
    "\t    nltk.tag.perceptron\t\n",
    "\t    nltk.tag.senna\t\n",
    "\t    nltk.tag.sequential\t\n",
    "\t    nltk.tag.stanford\t\n",
    "\t    nltk.tag.tnt\t\n",
    "\t    nltk.tag.util\t\n",
    "- \tnltk.tbl\t\n",
    "\t    nltk.tbl.demo\t\n",
    "\t    nltk.tbl.erroranalysis\t\n",
    "\t    nltk.tbl.feature\t\n",
    "\t    nltk.tbl.rule\t\n",
    "\t    nltk.tbl.template\t\n",
    "- \tnltk.test\t\n",
    "\t    nltk.test.all\t\n",
    "\t    nltk.test.childes_fixt\t\n",
    "\t    nltk.test.classify_fixt\t\n",
    "\t    nltk.test.conftest\t\n",
    "\t    nltk.test.discourse_fixt\t\n",
    "\t    nltk.test.gensim_fixt\t\n",
    "\t    nltk.test.gluesemantics_malt_fixt\t\n",
    "\t    nltk.test.inference_fixt\t\n",
    "\t    nltk.test.nonmonotonic_fixt\t\n",
    "\t    nltk.test.portuguese_en_fixt\t\n",
    "\t    nltk.test.probability_fixt\t\n",
    "\t    nltk.test.unit\t\n",
    "\t    nltk.test.unit.lm\t\n",
    "\t    nltk.test.unit.lm.test_counter\t\n",
    "\t    nltk.test.unit.lm.test_models\t\n",
    "\t    nltk.test.unit.lm.test_preprocessing\t\n",
    "\t    nltk.test.unit.lm.test_vocabulary\t\n",
    "\t    nltk.test.unit.test_aline\t\n",
    "\t    nltk.test.unit.test_bllip\t\n",
    "\t    nltk.test.unit.test_brill\t\n",
    "\t    nltk.test.unit.test_cfd_mutation\t\n",
    "\t    nltk.test.unit.test_cfg2chomsky\t\n",
    "\t    nltk.test.unit.test_chunk\t\n",
    "\t    nltk.test.unit.test_classify\t\n",
    "\t    nltk.test.unit.test_collocations\t\n",
    "\t    nltk.test.unit.test_concordance\t\n",
    "\t    nltk.test.unit.test_corenlp\t\n",
    "\t    nltk.test.unit.test_corpora\t\n",
    "\t    nltk.test.unit.test_corpus_views\t\n",
    "\t    nltk.test.unit.test_data\t\n",
    "\t    nltk.test.unit.test_disagreement\t\n",
    "\t    nltk.test.unit.test_distance\t\n",
    "\t    nltk.test.unit.test_downloader\t\n",
    "\t    nltk.test.unit.test_freqdist\t\n",
    "\t    nltk.test.unit.test_hmm\t\n",
    "\t    nltk.test.unit.test_json2csv_corpus\t\n",
    "\t    nltk.test.unit.test_json_serialization\t\n",
    "\t    nltk.test.unit.test_metrics\t\n",
    "\t    nltk.test.unit.test_naivebayes\t\n",
    "\t    nltk.test.unit.test_nombank\t\n",
    "\t    nltk.test.unit.test_pl196x\t\n",
    "\t    nltk.test.unit.test_pos_tag\t\n",
    "\t    nltk.test.unit.test_ribes\t\n",
    "\t    nltk.test.unit.test_rte_classify\t\n",
    "\t    nltk.test.unit.test_seekable_unicode_stream_reader\t\n",
    "\t    nltk.test.unit.test_senna\t\n",
    "\t    nltk.test.unit.test_stem\t\n",
    "\t    nltk.test.unit.test_tag\t\n",
    "\t    nltk.test.unit.test_tgrep\t\n",
    "\t    nltk.test.unit.test_tokenize\t\n",
    "\t    nltk.test.unit.test_twitter_auth\t\n",
    "\t    nltk.test.unit.test_util\t\n",
    "\t    nltk.test.unit.test_wordnet\t\n",
    "\t    nltk.test.unit.translate\t\n",
    "\t    nltk.test.unit.translate.test_bleu\t\n",
    "\t    nltk.test.unit.translate.test_gdfa\t\n",
    "\t    nltk.test.unit.translate.test_ibm1\t\n",
    "\t    nltk.test.unit.translate.test_ibm2\t\n",
    "\t    nltk.test.unit.translate.test_ibm3\t\n",
    "\t    nltk.test.unit.translate.test_ibm4\t\n",
    "\t    nltk.test.unit.translate.test_ibm5\t\n",
    "\t    nltk.test.unit.translate.test_ibm_model\t\n",
    "\t    nltk.test.unit.translate.test_meteor\t\n",
    "\t    nltk.test.unit.translate.test_nist\t\n",
    "\t    nltk.test.unit.translate.test_stack_decoder\t\n",
    "\tnltk.text\t\n",
    "\tnltk.tgrep\t\n",
    "- \tnltk.tokenize\t\n",
    "\t    nltk.tokenize.api\t\n",
    "\t    nltk.tokenize.casual\t\n",
    "\t    nltk.tokenize.destructive\t\n",
    "\t    nltk.tokenize.legality_principle\t\n",
    "\t    nltk.tokenize.mwe\t\n",
    "\t    nltk.tokenize.nist\t\n",
    "\t    nltk.tokenize.punkt\t\n",
    "\t    nltk.tokenize.regexp\t\n",
    "\t    nltk.tokenize.repp\t\n",
    "\t    nltk.tokenize.sexpr\t\n",
    "\t    nltk.tokenize.simple\t\n",
    "\t    nltk.tokenize.sonority_sequencing\t\n",
    "\t    nltk.tokenize.stanford\t\n",
    "\t    nltk.tokenize.stanford_segmenter\t\n",
    "\t    nltk.tokenize.texttiling\t\n",
    "\t    nltk.tokenize.toktok\t\n",
    "\t    nltk.tokenize.treebank\t\n",
    "\t    nltk.tokenize.util\t\n",
    "\tnltk.toolbox\t\n",
    "- \tnltk.translate\t\n",
    "\t    nltk.translate.api\t\n",
    "\t    nltk.translate.bleu_score\t\n",
    "\t    nltk.translate.chrf_score\t\n",
    "\t    nltk.translate.gale_church\t\n",
    "\t    nltk.translate.gdfa\t\n",
    "\t    nltk.translate.gleu_score\t\n",
    "\t    nltk.translate.ibm1\t\n",
    "\t    nltk.translate.ibm2\t\n",
    "\t    nltk.translate.ibm3\t\n",
    "\t    nltk.translate.ibm4\t\n",
    "\t    nltk.translate.ibm5\t\n",
    "\t    nltk.translate.ibm_model\t\n",
    "\t    nltk.translate.meteor_score\t\n",
    "\t    nltk.translate.metrics\t\n",
    "\t    nltk.translate.nist_score\t\n",
    "\t    nltk.translate.phrase_based\t\n",
    "\t    nltk.translate.ribes_score\t\n",
    "\t    nltk.translate.stack_decoder\t\n",
    "- \tnltk.tree\t\n",
    "\t    nltk.tree.immutable\t\n",
    "\t    nltk.tree.parented\t\n",
    "\t    nltk.tree.parsing\t\n",
    "\t    nltk.tree.prettyprinter\t\n",
    "\t    nltk.tree.probabilistic\t\n",
    "\t    nltk.tree.transforms\t\n",
    "\t    nltk.tree.tree\t\n",
    "\tnltk.treeprettyprinter\t\n",
    "\tnltk.treetransforms\t\n",
    "- \tnltk.twitter\t\n",
    "\t    nltk.twitter.api\t\n",
    "\t    nltk.twitter.common\t\n",
    "\t    nltk.twitter.twitter_demo\t\n",
    "\t    nltk.twitter.twitterclient\t\n",
    "\t    nltk.twitter.util\t\n",
    "\t \t\n",
    "\tu\t\n",
    "\tnltk.util\t\n",
    "\t \t\n",
    "\tw\t\n",
    "\tnltk.wsd\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
